---
title: "Aprendizaje Supervisado"
subtitle: "Curso JAE ICMAT 2021"
author: "Roi Naveiro"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: ["default", "custom.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      beforeInit: "macros.js"
    includes:
      before_body: mathjax.html
  keep_md: true
---

# El problema

* Tenemos disponibles datos con múltiples observaciones:
   
   * ejemplos (*examples*)
   * muestras (*samples*)

--

* Varias variables por observación, $x$:
  
  * predictores
  * atributos (*atributes*)
  * características (*features*)
  * covariables (*covariates*)
  * variables independientes
  * variables explicativas

--

* Una de ellas es de especial interés, $y$: 
  
  * variable respuesta
  * variable dependiente
  * objetivo (*target*)
  * salida (*output*)
  * etiqueta (*label*)
  
---

## Objetivos

  1. Obtener información sobre la **relación de asociación** entre las covariables y la variable dependiente  (**inferencia**)
  
    * ¿Qué variables influyen en la respuesta?
    * ¿Cuánto cambia la respuesta frente a un cambio en una covariable?
    * ...
    
  2. Predecir el valor de la variable respuesta para nuevas observaciones (**inferencia predictiva**)

  3. ¿Causalidad?


???

Información sobre la relación, por ej: qué variables son más relevantes

---

## Tipos de problemas

  1. Regresión, si $y \in \mathbb{R}^d$
  
  2. Clasificación, si la variable respuesta toma valores en un conjunto discreto no ordenado
  
  3. Otros: 
    
    * $y \in \mathbb  Q$
    
    * $y$ toma valores en un conjunto discreto ordenado
  
---

## Enfoque probabilístico

  1) **Modelización**: variable respuesta (dadas las covariables) es muestra iid de una distribución de probabilidad determinada, $p(y \vert x, w)$, parametrizada con $w$.
  
  2) **Estimación**: Dado un conjunto de entrenamiento $S = \{y_i, x_i\}_{i=1}^n$
  encontrar los "mejores" valores de $w$. Dos enfoques:
  * Frecuentista, maximizar verosimulitud
  $$
  w^* = \arg\max_{w}  \prod_i p(y_i \vert x_i, w)
  $$
  * Bayesiana, modelizar incertidumbre sobre los parámetros con un prior, y calcular posterior (dados los datos de entrenamiento)
  $$
  p(w | S) = \frac{p(S \vert w) p(w)}{p(S)}
  $$
  
  
---

## Enfoque probabilístico

  3) **Predicción**: dado un nuevo vector de covariables $x$, asignar un valor de respuesta $y$. Idea:
  
  $$
  y^* = \int u(y_D, y) p(y \vert x) dy
  $$
  
  * Ejercicio: ¿Cómo se calcula $p(y \vert x)$ en el paradigma Bayesiano?
  
  
  
---

## Regresión Lineal

Dado el conjunto de entrenamiento $S = \{y_i, x_i\}_{i=1}^n$


* Agrupamos todos los ejemplos de entrada $x_i$ en una matrix $\mathbf{X}$ de tamaño $n \times d$

* Agrupamos todas las salidas en un vector columna $y$ de tamaño $n \times 1$


Modelo $y \sim \mathcal{N} (w^T x, \sigma^2)$

Estimador de máxima verosimilitud:  $$\min_w\, ||y - \Xbf w||_2^2$$
Demuéstralo.


---
## Regresión Lineal

Gradiente: $$\nabla_w ||y - \Xbf w||_2^2 = 2\mathbf{X}^T(y - \mathbf{X}w) = \mathbf{X}^Ty - \mathbf{X}^T\mathbf{X}w$$

Minimizamos: $$\nabla_w ||y - \Xbf w||_2^2 = 0\quad \Rightarrow \quad  w^* = (\mathbf{X}^T\mathbf{X})^{-1} \mathbf{X}^T y$$

Recuperamos mínimos cuadrados ordinarios!


---
## Regresión Logística


* La salida $y$ es discreta, $y \in \{ 0, 1 \}$


* Modelo, $y$ sigue una distribución de Bernoulli con parámetro $p$ tal que
$$
p = \sigma(w^T x) = \frac{1}{1 + \exp(-w^T x)}
$$

* Estimador de máxima verosimilitud

$$\min_w \left[ -\sum_i y_i \log(\sigma(w^T x_i)) + (1-y_i) \log(1 -\sigma(w^T x_i)) \right] $$
Demuéstralo.

* Resolución Numérica
???

La función de pérdida log-loss es un poco distinta a como la vimos antes
  * Tiene un menos delante, porque queremos minimizar y no maximizar la verosimilitud
  * Propiedad de la función sigmoidea $\sigma(-x) = 1 - \sigma(x)$

---

## Generalized linear models (GLM)

* Generalización de la regresión lineal con otras distribuciones de la familia exponencial

* Componentes:
  
  * Distribución de $y$ con media $\mu$
  * Predictor lineal, $$g(\mu) = w^T x$$ donde $g(\cdot)$ es la función de media

* La función de media proporciona la relación entre la media de la distribución y el predictor lineal

* El inverso de la función de media, $g^{-1}(\cdot)$ se conoce con el nombre de **función de enlace**

---

##Ejemplo: Regresión logística

¿Cuál es su distribución y función de enlace?

???

* La función de enlace es la inversa de la anterior,

$$w^T x_i = g(\mu) = \ln\left(\frac{\mu}{1 - \mu}\right)$$

---

## Ejemplo: distribución de Poisson

* Esta distribución está indicada cuando queremos modelizar conteos

* Función de media

$$\mu = \exp(w^T x_i)$$

* Función de enlace

$$w^T x_i = \ln(\mu)$$

* Otras distribuciones posibles son la Gamma, Exponencial, Multinomial, etc.

---

## GLMs en R

* La función para ajustar modelos lineales generalizados es `glm()`

* Tiene los mismos argumentos principales que `lm()`, pero además tenemos que especificar la distribución de la variables dependiente con el parámetro `family`

* Por defecto se usa la función de enlace "canónica", pero esto se puede modificar (ver ayuda)

* Implementa el algoritmo IRLS (Newton-Raphson), que se puede generalizar para cualquier GLM donde la distribución pertenece a la familia exponencial

Ejemplo: regresión logística

```{r message=FALSE, warning=FALSE}
library(MASS)
fit <- glm(type ~ ., data=Pima.tr, family=binomial)
```

---

## Problemas (entre otros)

  * Poca flexibilidad (Sesgo grande), pues asumimos linealidad
  
  * Se puede solventar complicando el modelo (a mano) pero...
  
  * ... esto puede incrementar la varianza.
  
  * Trade-off

---

## Regularización

* Regresión *ridge* (*MSE* + regularización $l_2$): $$\min_w\, ||y - \Xbf w||_2^2 + \lambda ||w||_2^2$$


---

## Lasso: motivación

* Métodos de selección: 
   
   * ![:colorText green](modelos interpretables)
    
   * ![:colorText red](proceso discreto, las variables están incluidas o no)


* Regresión *ridge*: 

  * ![:colorText green](proceso continuo, todos los coeficientes se reducen)

  * ![:colorText red](rara vez son exactanente 0, modelos no interpretables)


* Lasso es una técnica intermedia:

  * ![:colorText green](reduce algunos coeficientes)
  
  * ![:colorText green](pone el resto a 0)

???

Modelos seleccion: variables entran o salen del modelo

---

## Lasso: formulación

* Problema optimización: $$\min_w\, || y - \Xbf w ||_2^2\quad \text{s.t. }\; ||w||_1 \leq t$$

* Equivalente: $$\min_w\, || y - \Xbf w ||_2^2 + \lambda ||w||_1$$

* $\lambda$ o $t$ son hiper-parámetros

  * $\uparrow \lambda$ o $\downarrow t$, se reducen los coeficientes (más regularización)

  * $\downarrow \lambda$ o $\uparrow t$, aumentan los coeficientes (menos regularización)
  
* $t$ suficientemente pequeño (o $\lambda$ suficientemente grande), algunos coeficientes = 0

