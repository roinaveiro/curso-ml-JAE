for (i in 1:100000){
Xt=2
T[i]=0
while(Xt<5){
T[i]=T[i]+rexp(1,aux[Xt])
Xt=sample(5,1,prob=P[Xt,])
}
}
#Cálculo de media y varianza
E=mean(T)
E2=mean(T^2)
V=E2-E^2
E
E2
V
sumafila
aux
#Determinación parámetros
lambda=0.03
alpha=0.005
mu=0.02
#Construcción matriz infinitesimal y encajada
aux=c(4*alpha,3/4*lambda+mu,lambda+2*mu,3/4*lambda+3*mu,4*mu)
Q=diag(-aux)
Q[1,2]=4*alpha
Q[5,4]=4*mu
for (i in 1:3){
Q[i+1,i]=i*mu
Q[i+1,i+2]=lambda*i*(4-i)/4
}
diag(Q)=0
P=Q/aux
P
Q
library(BayesVarSel)
data(Ozone35)
GibbsBvs(
formula = y ~ x4 + x5 + x6 + x7 + x8 + x9 + x10 ,
data = Ozone35,
prior.betas = "gZellner",
prior.models = "Constant",
n.iter = 10000,
init.model = "Full",
n.burnin = 500,
n.thin = 1,
time.test = TRUE,
seed = runif(1, 0, 16091956)
)
GibbsBvs(
formula = y ~ x4 + x5 + x6 + x7 + x8 + x9 + x10,
data = Ozone35,
prior.betas = "gZellner",
prior.models = "Constant",
n.iter = 10000,
init.model = "Full",
n.burnin = 500,
n.thin = 1,
time.test = TRUE,
seed = runif(1, 0, 16091956)
)
GibbsBvs(
formula = y ~ x4 + x5 + x6 + x7 + x8 + x9 + x10,
data = Ozone35
)
GibbsBvs(y ~ x4 + x5 + x6 + x7 + x8 + x9 + x10,
data = Ozone35
)
GibbsBvs(as.formula(y ~ x4 + x5 + x6 + x7 + x8 + x9 + x10),
data = Ozone35
)
GibbsBvs(as.formula(y ~ x4 + x5 + x6 + x7 + x8 + x9 + x10),
data = Ozone35
)
GibbsBvs(as.formula(y ~ x4 + x5 + x6 + x7 + x8 + x9 + x10), Ozone35
)
Bvs(as.formula(y ~ x4 + x5 + x6 + x7 + x8 + x9 + x10), Ozone35
)
Bvs(as.formula(y ~ x4 + x5 + x6 + x7 + x8 + x9 + x10), Ozone35)
prior.models = "Constant")
exact <- Bvs(as.formula(y ~ x4 + x5 + x6 + x7 + x8 + x9 + x10), Ozone35, prior.betas = "gZellner",
prior.models = "Constant")
exact
exact <- Bvs(as.formula(y ~ x4 + x5 + x6 + x7 + x8 + x9 + x10), Ozone35, prior.betas = "gZellner",
prior.models = "Constant")
exact
GibbsBvs(as.formula(y ~ x4 + x5 + x6 + x7 + x8 + x9 + x10), Ozone35)
GibbsBvs("y ~ x4 + x5 + x6 + x7 + x8 + x9 + x10", Ozone35)
Btest(
models,
Ozone35,
prior.betas = "gZellner",
prior.models = "Constant",
priorprobs = NULL,
null.model = NULL
)
models <- c(as.formula(y ~ 1),
as.formula(y ~ x4 + x5))
Btest(
models,
Ozone35,
prior.betas = "gZellner",
prior.models = "Constant",
priorprobs = NULL,
null.model = NULL
)
bf <- Btest(
models,
Ozone35,
prior.betas = "gZellner",
prior.models = "Constant",
priorprobs = NULL,
null.model = NULL
)
bf$BFi0
bf$BFi0[2]
exact <- Bvs(as.formula(y ~ x4 + x5 + x6 + x7 + x8 + x9 + x10), Ozone35, prior.betas = "gZellner",
prior.models = "Constant")
exact
models <- c(as.formula(y ~ 1),
as.formula(y ~ x6 + x7 + x8))
bf <- Btest(
models,
Ozone35,
prior.betas = "gZellner",
prior.models = "Constant",
priorprobs = NULL,
null.model = NULL
)
bf
exact <- Bvs(as.formula(y ~ x6 + x7 + x8 + x9), Ozone35, prior.betas = "gZellner",
prior.models = "Constant")
exact
exact <- Bvs(as.formula(y ~ x6 + x7 + x8), Ozone35, prior.betas = "gZellner",
prior.models = "Constant")
exact
library(MASS)
x1 = rnorm(100)
x1
x2 = rnorm(100)
x1 = rnorm(100, mean = 2)
x2 = rnorm(100, mean = 5)
library(MASS)
x1 = rnorm(100, mean = 2)
x2 = rnorm(100, mean = 5)
y1 = 3*x1 + 2*x2 + rnorm(100, mean=0, sd=0.3)
y2 = 3*x1 + 2*x2 + rnorm(100, mean=0, sd=0.2)
mlm1 <- lm(cbind(y1, y2) ~ x1 + x2)
summary(mlm1)
y1 = 3*x1 + 2*x2 + rnorm(100, mean=0, sd=0.3)
y2 = 5*x1 + 4*x2 + rnorm(100, mean=0, sd=0.2)
mlm1 <- lm(cbind(y1, y2) ~ x1 + x2)
summary(mlm1)
m1 = lm(y1 ~ x1 + x2)
summary(m1)
xaringan:::inf_mr()
library(ggplot2)
library(ggplot2)
gen_dat <- function(n = 100, b = 0, sigma2 = 1, seed = 1) {
set.seed(seed)
p <- length(b)
X <- replicate(p, rnorm(n))
y <- X %*% t(b) + rnorm(n, 0, sqrt(sigma2))
list('y' = y, 'X' = X)
}
dat <- gen_dat(n = 100, b = 0.3, sigma2 = 1)
samples <- ss_regress_univ(dat$y, dat$X)
head(samples)
dat
#'
#' @param y: vector of responses
#' @param x: vector of predictor values
#' @param nr_samples: indicates number of samples drawn
#' @param a1: parameter a1 of Gamma prior on variance sigma2e
#' @param a2: parameter a2 of Gamma prior on variance sigma2e
#' @param theta: parameter of prior over mixture weight
#' @param burnin: number of samples we discard ('burnin samples')
#'
#' @returns matrix of posterior samples from parameters pi, beta, tau2, sigma2e, theta
ss_regress_univ <- function(
y, x, nr_samples = 4000, a1 = .01, a2 = .01,
theta = 0.5, s = 1/2, a = 1, b = 1, nr_burnin = round(nr_samples / 4, 2)
) {
# res is where we store the posterior samples
res <- matrix(NA, nrow = nr_samples, ncol = 5)
colnames(res) <- c('pi', 'beta', 'sigma2', 'tau2', 'theta')
# take the MLE estimate as the values for the first sample
m <- lm(y ~ x - 1)
res[1, ] <- c(0, coef(m), var(predict(m) - y), 1, .5)
# compute these quantities only once
n <- length(y)
sum_xy <- sum(x*y)
sum_x2 <- sum(x^2)
# we start running the Gibbs sampler
for (i in seq(2, nr_samples)) {
# first, get all the values of the previous time point
pi_prev <- res[i-1, 1]
beta_prev <- res[i-1, 2]
sigma2_prev <- res[i-1, 3]
tau2_prev <- res[i-1, 4]
theta_prev <- res[i-1, 5]
## Start sampling from the conditional posterior distributions
##############################################################
# sample theta from a Beta
theta_new <- rbeta(1, a + pi_prev, b + 1 - pi_prev)
# sample sigma2e from an Inverse Gamma
sigma2_new <- 1 / rgamma(1, a1 + n/2, a2 + sum((y - x*beta_prev)^2) / 2)
# sample tau2 from an Inverse Gamma
tau2_new <- 1 / rgamma(1, 1/2 + 1/2 * pi_prev, s^2/2 + beta_prev^2 / (2*sigma2_new))
# store this as a variable since it gets computed very often
cond_var <- sum_x2 + 1/tau2_new
# sample beta from a Gaussian
beta_mu <- sum_xy / cond_var
beta_var <- sigma2_new / cond_var
beta_new <- rnorm(1, beta_mu, sqrt(beta_var))
# compute chance parameter of the conditional posterior of pi (Bernoulli)
l0 <- log(1 - theta_new)
l1 <- (
log(theta_new) - .5 * log(tau2_new*sigma2_new) +
sum_xy^2 / (2*sigma2_new*cond_var) + .5 * log(beta_var)
)
# sample pi from a Bernoulli
pi_new <- rbinom(1, 1, exp(l1) / (exp(l1) + exp(l0)))
# add new samples
res[i, ] <- c(pi_new, beta_new * pi_new, sigma2_new, tau2_new, theta_new)
}
# remove the first nr_burnin number of samples
res[-seq(nr_burnin), ]
}
replicate(1, rnorm(2))
replicate(1, rnorm(10))
replicate(2, rnorm(10))
dat <- gen_dat(n = 100, b = 0.3, sigma2 = 1)
samples <- ss_regress_univ(dat$y, dat$X)
head(samples)
apply(samples, 2, mean)
dat <- gen_dat(n = 1000, b = 0.3, sigma2 = 1)
samples <- ss_regress_univ(dat$y, dat$X)
apply(samples, 2, mean)
library(ggplot2)
#' Spike-and-Slab Regression using Gibbs Sampling for p = 1 predictors
#'
#' @param y: vector of responses
#' @param x: vector of predictor values
#' @param nr_samples: indicates number of samples drawn
#' @param a1: parameter a1 of Gamma prior on variance sigma2e
#' @param a2: parameter a2 of Gamma prior on variance sigma2e
#' @param theta: parameter of prior over mixture weight
#' @param burnin: number of samples we discard ('burnin samples')
#'
#' @returns matrix of posterior samples from parameters pi, beta, tau2, sigma2e, theta
#' Spike-and-Slab Regression using Gibbs Sampling for p > 1 predictors
#'
#' @param y: vector of responses
#' @param X: matrix of predictor values
#' @param nr_samples: indicates number of samples drawn
#' @param a1: parameter a1 of Gamma prior on variance sigma2e
#' @param a2: parameter a2 of Gamma prior on variance sigma2e
#' @param theta: parameter of prior over mixture weight
#' @param burnin: number of samples we discard ('burnin samples')
#'
#' @returns matrix of posterior samples from parameters pi, beta, tau2, sigma2e, theta
ss_regress <- function(
y, X, a1 = .01, a2 = .01, theta = .5,
a = 1, b = 1, s = 1/2, nr_samples = 6000, nr_burnin = round(nr_samples / 4, 2)
) {
p <- ncol(X)
n <- nrow(X)
# res is where we store the posterior samples
res <- matrix(NA, nrow = nr_samples, ncol = 2*p + 1 + 1 + 1)
colnames(res) <- c(
paste0('pi', seq(p)),
paste0('beta', seq(p)),
'sigma2', 'tau2', 'theta'
)
# take the MLE estimate as the values for the first sample
m <- lm(y ~ X - 1)
res[1, ] <- c(rep(0, p), coef(m), var(predict(m) - y), 1, .5)
# compute only once
XtX <- t(X) %*% X
Xty <- t(X) %*% y
# we start running the Gibbs sampler
for (i in seq(2, nr_samples)) {
# first, get all the values of the previous time point
pi_prev <- res[i-1, seq(p)]
beta_prev <- res[i-1, seq(p + 1, 2*p)]
sigma2_prev <- res[i-1, ncol(res) - 2]
tau2_prev <- res[i-1, ncol(res) - 1]
theta_prev <- res[i-1, ncol(res)]
## Start sampling from the conditional posterior distributions
##############################################################
# sample theta from a Beta
theta_new <- rbeta(1, a + sum(pi_prev), b + sum(1 - pi_prev))
# sample sigma2e from an Inverse-Gamma
err <- y - X %*% beta_prev
sigma2_new <- 1 / rgamma(1, a1 + n/2, a2 + t(err) %*% err / 2)
# sample tau2 from an Inverse Gamma
tau2_new <- 1 / rgamma(
1, 1/2 + 1/2 * sum(pi_prev),
s^2/2 + t(beta_prev) %*% beta_prev / (2*sigma2_new)
)
# sample beta from multivariate Gaussian
beta_cov <- qr.solve((1/sigma2_new) * XtX + diag(1/(tau2_new*sigma2_new), p))
beta_mean <- beta_cov %*% Xty * (1/sigma2_new)
beta_new <- mvtnorm::rmvnorm(1, beta_mean, beta_cov)
# sample each pi_j in random order
for (j in sample(seq(p))) {
# get the betas for which beta_j is zero
pi0 <- pi_prev
pi0[j] <- 0
bp0 <- t(beta_new * pi0)
# compute the z variables and the conditional variance
xj <- X[, j]
z <- y - X %*% bp0
cond_var <- sum(xj^2) + 1/tau2_new
# compute chance parameter of the conditional posterior of pi_j (Bernoulli)
l0 <- log(1 - theta_new)
l1 <- (
log(theta_new) - .5 * log(tau2_new*sigma2_new) +
sum(xj*z)^2 / (2*sigma2_new*cond_var) + .5 * log(sigma2_new / cond_var)
)
# sample pi_j from a Bernoulli
pi_prev[j] <- rbinom(1, 1, exp(l1) / (exp(l1) + exp(l0)))
}
pi_new <- pi_prev
# add new samples
res[i, ] <- c(pi_new, beta_new*pi_new, sigma2_new, tau2_new, theta_new)
}
# remove the first nr_burnin number of samples
res[-seq(nr_burnin), ]
}
library('doParallel')
registerDoParallel(cores = 4)
#' Calls the ss_regress function in parallel
#'
#' @params same as ss_regress
#' @params nr_cores: numeric, number of cores to run ss_regress in parallel
#' @returns a list with nr_cores entries which are posterior samples
ss_regressm <- function(
y, X, a1 = .01, a2 = .01, theta = .5,
a = 1, b = 1, s = 1/2, nr_samples = 6000,
nr_burnin = round(nr_samples / 4, 2), nr_cores = 4
) {
samples <- foreach(i = seq(nr_cores), .combine = rbind) %dopar% {
ss_regress(
y = y, X = X, a1 = a1, a2 = a2, theta = theta,
a = a, b = b, s = s, nr_samples = nr_samples,
nr_burnin = nr_burnin
)
}
samples
}
data(attitude)
head(attitude)
std <- function(x) (x - mean(x)) / sd(x)
attitude_z <- apply(attitude, 2, std)
yz <- attitude_z[, 1]
Xz <- attitude_z[, -1]
samples <- ss_regressm(
y = yz, X = Xz, a1 = .01, a2 = .01,
a = 1, b = 1, s = 1/2, nr_cores = 4, nr_samples = 4000
)
post_means <- apply(samples, 2, mean)
res_table <- cbind(
post_means[grepl('beta', names(post_means))],
post_means[grepl('pi', names(post_means))]
)
rownames(res_table) <- colnames(Xz)
colnames(res_table) <- c('Post. Mean', 'Post. Inclusion')
round(res_table, 3)
options(java.parameters = "-Xmx5g")
library(bartMachine)
library(readxl)
x <- rnorm(100, m=10, sd=1)
y <- 3*sin(x) + rnorm(100, 0, 0.1)
plot(x, y)
X_train <- data.frame(x)
y_train <- y
bart_machine <- bartMachine(X = X_train, y = y_train,
num_trees = 200,
num_burn_in = 1000,
num_iterations_after_burn_in = 10000,
alpha = 0.95, beta = 2, k = 2, q = 0.99, nu = 3)
plot_y_vs_yhat(bart_machine, prediction_intervals = TRUE, interval_confidence_level = 0.8)
plot_y_vs_yhat(bart_machine, prediction_intervals = TRUE, interval_confidence_level = 0.9)
plot(x, y)
X_test <- data.frame(c(10, 11, 8, 12, 0.0, 20.0))
pred <- calc_prediction_intervals(bart_machine, X_test[1,],
pi_conf = 0.95, num_samples_per_data_point = 10000)
X_test <- data.frame(c(10, 11, 8, 12, 0.0, 20.0))
X_test
X_train
x_t <- c(10, 11, 8, 12, 0.0, 20.0)
X_test <- data.frame(x_t)
pred <- calc_prediction_intervals(bart_machine, X_test[1,],
pi_conf = 0.95, num_samples_per_data_point = 10000)
x <- c(10, 11, 8, 12, 0.0, 20.0)
X_test <- data.frame(x_t)
pred <- calc_prediction_intervals(bart_machine, X_test[1,],
pi_conf = 0.95, num_samples_per_data_point = 10000)
x <- c(10, 11, 8, 12, 0.0, 20.0)
X_test <- data.frame(x)
pred <- calc_prediction_intervals(bart_machine, X_test[1,],
pi_conf = 0.95, num_samples_per_data_point = 10000)
X_test
head(X_train)
pred <- calc_prediction_intervals(bart_machine, X_test[1],
pi_conf = 0.95, num_samples_per_data_point = 10000)
x <- c(10, 11, 8, 12, 0.0, 20.0)
X_test <- data.frame(x)
pred <- calc_prediction_intervals(bart_machine, X_test[1],
pi_conf = 0.95, num_samples_per_data_point = 10000)
pred$interval
X_test[1]
X_test[1,]
pred <- calc_prediction_intervals(bart_machine, X_test[1,],
pi_conf = 0.95, num_samples_per_data_point = 10000)
x <- c(10, 11, 8, 12, 0.0, 20.0)
X_test <- data.frame(x)
pred <- calc_prediction_intervals(bart_machine, X_test,
pi_conf = 0.95, num_samples_per_data_point = 10000)
X_test
pred
pred$interval
3*sin(10)
3*sin(0)
1:20
1:20:0.5
seq(1,20, 0.5)
0
seq(0,20, 0.5)
X_test <- data.frame(seq(0,20, 0.5))
pred <- calc_prediction_intervals(bart_machine, X_test,
pi_conf = 0.95, num_samples_per_data_point = 10000)
x <- seq(0,20, 0.5)
X_test <- data.frame(x)
pred <- calc_prediction_intervals(bart_machine, X_test,
pi_conf = 0.95, num_samples_per_data_point = 10000)
y_pred <- apply(pred$all_prediction_samples, 1, mean)
y_pred
plot(x, y_pred)
pred$interval
pred <- calc_credible_intervals(bart_machine, X_test,
pi_conf = 0.95, num_samples_per_data_point = 10000)
pred <- calc_credible_intervals(bart_machine, X_test,
pi_conf = 0.95)
pred <- calc_credible_intervals(bart_machine, X_test)
pred
?calc_credible_intervals
pred <- calc_credible_intervals(bart_machine, X_test, ci_conf = 0.95)
pred
pollen
library(class)
n <- nrow(iris)
# muestreo aleatorio
idx <- sample(n, n*0.75)
# partir en conjuntos de entrenamiento y test
train <- iris[idx, ]
test <- iris[-idx, ]
# separar variables indenpendientes de la clase (variable respuesta)
# entrenamiento
y_train <- train[,  5]
X_train <- train[, -5]
# test
y_test <- test[,  5]
X_test <- test[, -5]
# modelo knn
y_pred <- knn(X_train, X_test, y_train, k=3)
# tasa acierto
mean(y_test == y_pred)*100
# tasa acierto
mean(y_test == y_pred)*100
library(ElemStatLearn)
install.packages("ElemSattLearn")
install.packages("ElemStatLearn")
install.packages("ElemStatLearn")
install.packages("~/Downloads/ElemStatLearn_2015.6.26.tar.gz", repos = NULL, type = "source")
library(ElemStatLearn)
require(class)
plot_knn <- function(k) {
x <- mixture.example$x
g <- mixture.example$y
xnew <- mixture.example$xnew
mod15 <- knn(x, xnew, g, k=k, prob=TRUE)
prob <- attr(mod15, "prob")
prob <- ifelse(mod15=="1", prob, 1-prob)
px1 <- mixture.example$px1
px2 <- mixture.example$px2
prob15 <- matrix(prob, length(px1), length(px2))
par(mar=rep(2,4))
contour(px1, px2, prob15, levels=0.5, labels="", xlab="", ylab="", main=
paste("Vecinos próximos, k=", k, sep=""), axes=FALSE)
points(x, col=ifelse(g==1, "coral", "cornflowerblue"))
gd <- expand.grid(x=px1, y=px2)
points(gd, pch=".", cex=1.2, col=ifelse(prob15>0.5, "coral", "cornflowerblue"))
box()
}
plot_knn(15)
xaringan:::inf_mr()
xaringan:::inf_mr()
install.packages("readImage")
# NOT RUN {
# read a sample file (R logo)
img <- readJPEG(system.file("img", "Rlogo.jpg", package="jpeg"))
getcwd()
setwd("~/MEGA/docencia/curso-ml-JAE/docs/sesion_03/excercises")
readImage("riaño.jpg")
install.packages("OpenImageR")
library(OpenImageR)
readImage("riaño.jpg")
a = readImage("riaño.jpg")
a
xaringan:::inf_mr()
