# sample sigma2e from an Inverse Gamma
sigma2_new <- 1 / rgamma(1, a1 + n/2, a2 + sum((y - x*beta_prev)^2) / 2)
# sample tau2 from an Inverse Gamma
tau2_new <- 1 / rgamma(1, 1/2 + 1/2 * pi_prev, s^2/2 + beta_prev^2 / (2*sigma2_new))
# store this as a variable since it gets computed very often
cond_var <- sum_x2 + 1/tau2_new
# sample beta from a Gaussian
beta_mu <- sum_xy / cond_var
beta_var <- sigma2_new / cond_var
beta_new <- rnorm(1, beta_mu, sqrt(beta_var))
# compute chance parameter of the conditional posterior of pi (Bernoulli)
l0 <- log(1 - theta_new)
l1 <- (
log(theta_new) - .5 * log(tau2_new*sigma2_new) +
sum_xy^2 / (2*sigma2_new*cond_var) + .5 * log(beta_var)
)
# sample pi from a Bernoulli
pi_new <- rbinom(1, 1, exp(l1) / (exp(l1) + exp(l0)))
# add new samples
res[i, ] <- c(pi_new, beta_new * pi_new, sigma2_new, tau2_new, theta_new)
}
# remove the first nr_burnin number of samples
res[-seq(nr_burnin), ]
}
replicate(1, rnorm(2))
replicate(1, rnorm(10))
replicate(2, rnorm(10))
dat <- gen_dat(n = 100, b = 0.3, sigma2 = 1)
samples <- ss_regress_univ(dat$y, dat$X)
head(samples)
apply(samples, 2, mean)
dat <- gen_dat(n = 1000, b = 0.3, sigma2 = 1)
samples <- ss_regress_univ(dat$y, dat$X)
apply(samples, 2, mean)
library(ggplot2)
#' Spike-and-Slab Regression using Gibbs Sampling for p = 1 predictors
#'
#' @param y: vector of responses
#' @param x: vector of predictor values
#' @param nr_samples: indicates number of samples drawn
#' @param a1: parameter a1 of Gamma prior on variance sigma2e
#' @param a2: parameter a2 of Gamma prior on variance sigma2e
#' @param theta: parameter of prior over mixture weight
#' @param burnin: number of samples we discard ('burnin samples')
#'
#' @returns matrix of posterior samples from parameters pi, beta, tau2, sigma2e, theta
#' Spike-and-Slab Regression using Gibbs Sampling for p > 1 predictors
#'
#' @param y: vector of responses
#' @param X: matrix of predictor values
#' @param nr_samples: indicates number of samples drawn
#' @param a1: parameter a1 of Gamma prior on variance sigma2e
#' @param a2: parameter a2 of Gamma prior on variance sigma2e
#' @param theta: parameter of prior over mixture weight
#' @param burnin: number of samples we discard ('burnin samples')
#'
#' @returns matrix of posterior samples from parameters pi, beta, tau2, sigma2e, theta
ss_regress <- function(
y, X, a1 = .01, a2 = .01, theta = .5,
a = 1, b = 1, s = 1/2, nr_samples = 6000, nr_burnin = round(nr_samples / 4, 2)
) {
p <- ncol(X)
n <- nrow(X)
# res is where we store the posterior samples
res <- matrix(NA, nrow = nr_samples, ncol = 2*p + 1 + 1 + 1)
colnames(res) <- c(
paste0('pi', seq(p)),
paste0('beta', seq(p)),
'sigma2', 'tau2', 'theta'
)
# take the MLE estimate as the values for the first sample
m <- lm(y ~ X - 1)
res[1, ] <- c(rep(0, p), coef(m), var(predict(m) - y), 1, .5)
# compute only once
XtX <- t(X) %*% X
Xty <- t(X) %*% y
# we start running the Gibbs sampler
for (i in seq(2, nr_samples)) {
# first, get all the values of the previous time point
pi_prev <- res[i-1, seq(p)]
beta_prev <- res[i-1, seq(p + 1, 2*p)]
sigma2_prev <- res[i-1, ncol(res) - 2]
tau2_prev <- res[i-1, ncol(res) - 1]
theta_prev <- res[i-1, ncol(res)]
## Start sampling from the conditional posterior distributions
##############################################################
# sample theta from a Beta
theta_new <- rbeta(1, a + sum(pi_prev), b + sum(1 - pi_prev))
# sample sigma2e from an Inverse-Gamma
err <- y - X %*% beta_prev
sigma2_new <- 1 / rgamma(1, a1 + n/2, a2 + t(err) %*% err / 2)
# sample tau2 from an Inverse Gamma
tau2_new <- 1 / rgamma(
1, 1/2 + 1/2 * sum(pi_prev),
s^2/2 + t(beta_prev) %*% beta_prev / (2*sigma2_new)
)
# sample beta from multivariate Gaussian
beta_cov <- qr.solve((1/sigma2_new) * XtX + diag(1/(tau2_new*sigma2_new), p))
beta_mean <- beta_cov %*% Xty * (1/sigma2_new)
beta_new <- mvtnorm::rmvnorm(1, beta_mean, beta_cov)
# sample each pi_j in random order
for (j in sample(seq(p))) {
# get the betas for which beta_j is zero
pi0 <- pi_prev
pi0[j] <- 0
bp0 <- t(beta_new * pi0)
# compute the z variables and the conditional variance
xj <- X[, j]
z <- y - X %*% bp0
cond_var <- sum(xj^2) + 1/tau2_new
# compute chance parameter of the conditional posterior of pi_j (Bernoulli)
l0 <- log(1 - theta_new)
l1 <- (
log(theta_new) - .5 * log(tau2_new*sigma2_new) +
sum(xj*z)^2 / (2*sigma2_new*cond_var) + .5 * log(sigma2_new / cond_var)
)
# sample pi_j from a Bernoulli
pi_prev[j] <- rbinom(1, 1, exp(l1) / (exp(l1) + exp(l0)))
}
pi_new <- pi_prev
# add new samples
res[i, ] <- c(pi_new, beta_new*pi_new, sigma2_new, tau2_new, theta_new)
}
# remove the first nr_burnin number of samples
res[-seq(nr_burnin), ]
}
library('doParallel')
registerDoParallel(cores = 4)
#' Calls the ss_regress function in parallel
#'
#' @params same as ss_regress
#' @params nr_cores: numeric, number of cores to run ss_regress in parallel
#' @returns a list with nr_cores entries which are posterior samples
ss_regressm <- function(
y, X, a1 = .01, a2 = .01, theta = .5,
a = 1, b = 1, s = 1/2, nr_samples = 6000,
nr_burnin = round(nr_samples / 4, 2), nr_cores = 4
) {
samples <- foreach(i = seq(nr_cores), .combine = rbind) %dopar% {
ss_regress(
y = y, X = X, a1 = a1, a2 = a2, theta = theta,
a = a, b = b, s = s, nr_samples = nr_samples,
nr_burnin = nr_burnin
)
}
samples
}
data(attitude)
head(attitude)
std <- function(x) (x - mean(x)) / sd(x)
attitude_z <- apply(attitude, 2, std)
yz <- attitude_z[, 1]
Xz <- attitude_z[, -1]
samples <- ss_regressm(
y = yz, X = Xz, a1 = .01, a2 = .01,
a = 1, b = 1, s = 1/2, nr_cores = 4, nr_samples = 4000
)
post_means <- apply(samples, 2, mean)
res_table <- cbind(
post_means[grepl('beta', names(post_means))],
post_means[grepl('pi', names(post_means))]
)
rownames(res_table) <- colnames(Xz)
colnames(res_table) <- c('Post. Mean', 'Post. Inclusion')
round(res_table, 3)
options(java.parameters = "-Xmx5g")
library(bartMachine)
library(readxl)
x <- rnorm(100, m=10, sd=1)
y <- 3*sin(x) + rnorm(100, 0, 0.1)
plot(x, y)
X_train <- data.frame(x)
y_train <- y
bart_machine <- bartMachine(X = X_train, y = y_train,
num_trees = 200,
num_burn_in = 1000,
num_iterations_after_burn_in = 10000,
alpha = 0.95, beta = 2, k = 2, q = 0.99, nu = 3)
plot_y_vs_yhat(bart_machine, prediction_intervals = TRUE, interval_confidence_level = 0.8)
plot_y_vs_yhat(bart_machine, prediction_intervals = TRUE, interval_confidence_level = 0.9)
plot(x, y)
X_test <- data.frame(c(10, 11, 8, 12, 0.0, 20.0))
pred <- calc_prediction_intervals(bart_machine, X_test[1,],
pi_conf = 0.95, num_samples_per_data_point = 10000)
X_test <- data.frame(c(10, 11, 8, 12, 0.0, 20.0))
X_test
X_train
x_t <- c(10, 11, 8, 12, 0.0, 20.0)
X_test <- data.frame(x_t)
pred <- calc_prediction_intervals(bart_machine, X_test[1,],
pi_conf = 0.95, num_samples_per_data_point = 10000)
x <- c(10, 11, 8, 12, 0.0, 20.0)
X_test <- data.frame(x_t)
pred <- calc_prediction_intervals(bart_machine, X_test[1,],
pi_conf = 0.95, num_samples_per_data_point = 10000)
x <- c(10, 11, 8, 12, 0.0, 20.0)
X_test <- data.frame(x)
pred <- calc_prediction_intervals(bart_machine, X_test[1,],
pi_conf = 0.95, num_samples_per_data_point = 10000)
X_test
head(X_train)
pred <- calc_prediction_intervals(bart_machine, X_test[1],
pi_conf = 0.95, num_samples_per_data_point = 10000)
x <- c(10, 11, 8, 12, 0.0, 20.0)
X_test <- data.frame(x)
pred <- calc_prediction_intervals(bart_machine, X_test[1],
pi_conf = 0.95, num_samples_per_data_point = 10000)
pred$interval
X_test[1]
X_test[1,]
pred <- calc_prediction_intervals(bart_machine, X_test[1,],
pi_conf = 0.95, num_samples_per_data_point = 10000)
x <- c(10, 11, 8, 12, 0.0, 20.0)
X_test <- data.frame(x)
pred <- calc_prediction_intervals(bart_machine, X_test,
pi_conf = 0.95, num_samples_per_data_point = 10000)
X_test
pred
pred$interval
3*sin(10)
3*sin(0)
1:20
1:20:0.5
seq(1,20, 0.5)
0
seq(0,20, 0.5)
X_test <- data.frame(seq(0,20, 0.5))
pred <- calc_prediction_intervals(bart_machine, X_test,
pi_conf = 0.95, num_samples_per_data_point = 10000)
x <- seq(0,20, 0.5)
X_test <- data.frame(x)
pred <- calc_prediction_intervals(bart_machine, X_test,
pi_conf = 0.95, num_samples_per_data_point = 10000)
y_pred <- apply(pred$all_prediction_samples, 1, mean)
y_pred
plot(x, y_pred)
pred$interval
pred <- calc_credible_intervals(bart_machine, X_test,
pi_conf = 0.95, num_samples_per_data_point = 10000)
pred <- calc_credible_intervals(bart_machine, X_test,
pi_conf = 0.95)
pred <- calc_credible_intervals(bart_machine, X_test)
pred
?calc_credible_intervals
pred <- calc_credible_intervals(bart_machine, X_test, ci_conf = 0.95)
pred
pollen
library(class)
n <- nrow(iris)
# muestreo aleatorio
idx <- sample(n, n*0.75)
# partir en conjuntos de entrenamiento y test
train <- iris[idx, ]
test <- iris[-idx, ]
# separar variables indenpendientes de la clase (variable respuesta)
# entrenamiento
y_train <- train[,  5]
X_train <- train[, -5]
# test
y_test <- test[,  5]
X_test <- test[, -5]
# modelo knn
y_pred <- knn(X_train, X_test, y_train, k=3)
# tasa acierto
mean(y_test == y_pred)*100
# tasa acierto
mean(y_test == y_pred)*100
library(ElemStatLearn)
install.packages("ElemSattLearn")
install.packages("ElemStatLearn")
install.packages("ElemStatLearn")
install.packages("~/Downloads/ElemStatLearn_2015.6.26.tar.gz", repos = NULL, type = "source")
library(ElemStatLearn)
require(class)
plot_knn <- function(k) {
x <- mixture.example$x
g <- mixture.example$y
xnew <- mixture.example$xnew
mod15 <- knn(x, xnew, g, k=k, prob=TRUE)
prob <- attr(mod15, "prob")
prob <- ifelse(mod15=="1", prob, 1-prob)
px1 <- mixture.example$px1
px2 <- mixture.example$px2
prob15 <- matrix(prob, length(px1), length(px2))
par(mar=rep(2,4))
contour(px1, px2, prob15, levels=0.5, labels="", xlab="", ylab="", main=
paste("Vecinos próximos, k=", k, sep=""), axes=FALSE)
points(x, col=ifelse(g==1, "coral", "cornflowerblue"))
gd <- expand.grid(x=px1, y=px2)
points(gd, pch=".", cex=1.2, col=ifelse(prob15>0.5, "coral", "cornflowerblue"))
box()
}
plot_knn(15)
xaringan:::inf_mr()
xaringan:::inf_mr()
install.packages("readImage")
# NOT RUN {
# read a sample file (R logo)
img <- readJPEG(system.file("img", "Rlogo.jpg", package="jpeg"))
getcwd()
setwd("~/MEGA/docencia/curso-ml-JAE/docs/sesion_03/excercises")
readImage("riaño.jpg")
install.packages("OpenImageR")
library(OpenImageR)
readImage("riaño.jpg")
a = readImage("riaño.jpg")
a
xaringan:::inf_mr()
knitr::opts_chunk$set(echo = TRUE)
test  = load_image_file("src/t10k-images.idx3-ubyte")
knitr::opts_chunk$set(echo = TRUE)
load("gss.RData")
load("data/gss.RData")
#### Fig 3.9
y2<-gss$CHILDS[gss$FEMALE==1 &  gss$YEAR>=1990  & gss$AGE==40 & gss$DEG>=3 ]
y1<-gss$CHILDS[gss$FEMALE==1 &  gss$YEAR>=1990  & gss$AGE==40 & gss$DEG<3 ]
y2<-y2[!is.na(y2)]
y1<-y1[!is.na(y1)]
par(mar=c(3,3,1,1),mgp=c(1.75,.75,0))
par(mfrow=c(1,2))
set.seed(1)
n1<-length(y1) ; n2<-length(y2)
s1<-sum(y1)
s2<-sum(y2)
par(mfrow=c(1,2),mar=c(3,3,1,1),mgp=c(1.75,.75,0))
plot(table(y1), type="h",xlab=expression(italic(y)),ylab=expression(italic(n[1](y))),col=gray(.5) ,lwd=3)
mtext("Less than bachelor's",side=3)
plot(table(y2), type="h",xlab=expression(italic(y)),ylab=expression(italic(n[2](y))),col=gray(0),lwd=3)
mtext("Bachelor's or higher",side=3,lwd=3)
par(mar=c(3,3,1,1),mgp=c(1.75,.75,0))
par(mfrow=c(1,2))
a<-2
b<-1
xtheta<-seq(0,5,length=1000)
plot(xtheta,dgamma(xtheta,a+s1,b+n1),type="l",col=gray(.5),xlab=expression(theta),
ylab=expression(paste(italic("p("),theta,"|",y[1],"...",y[n],")",sep="")))
lines(xtheta,dgamma(xtheta,a+s2,b+n2),col=gray(0),lwd=2)
lines(xtheta,dgamma(xtheta,a,b),type="l",lty=2,lwd=2)
abline(h=0,col="black")
y<-(0:12)
plot(y-.1, dnbinom(y, size=(a+s1), mu=(a+s1)/(b+n1)) , col=gray(.5) ,type="h",
ylab=expression(paste(italic("p("),y[n+1],"|",y[1],"...",y[n],")",sep="")),
xlab=expression(italic(y[n+1])),ylim=c(0,.35),lwd=3)
points(y+.1, dnbinom(y, size=(a+s2), mu=(a+s2)/(b+n2)) , col=gray(0) ,type="h",lwd=3)
legend(1,.375,legend=c("Less than bachelor's","Bachelor's or higher"),bty="n",
lwd=c(3,3),col=c(gray(.5),gray(0)))
a<−2 ; b<−1
a <- 2 ; b <- 1
n1 <- 111 ; sy1 <- 217
n2 <- 44 ;  sy2 <- 66
print("Posterior mean women without college degree", ( a+sy1 ) / ( b+n1 ) )
( a+sy1 − 1)/(b+n1 ) # post mode
a <- 2 ; b <- 1
n1 <- 111 ; sy1 <- 217
n2 <- 44 ;  sy2 <- 66
print("Posterior mean women without college degree", ( a+sy1 ) / ( b+n1 ) )
( a+sy1 - 1)/(b+n1 ) # post mode
qgamma( c ( . 0 2 5 , . 9 7 5 ) , a+sy1 , b+n1 )
a <- 2 ; b <- 1
n1 <- 111 ; sy1 <- 217
n2 <- 44 ;  sy2 <- 66
print("Posterior mean women without college degree", ( a+sy1 ) / ( b+n1 ) )
( a+sy1 - 1)/(b+n1 ) # post mode
qgamma( c ( 0.025 , 0.975 ) , a+sy1 , b+n1 )
( a+sy2 ) / ( b+n2 )
( a+sy2 - 1)/(b+n2 )
qgamma( c ( 0.025 , 0.975 ) , a+sy2 , b+n2 )
a <- 2 ; b <- 1
n1 <- 111 ; sy1 <- 217
n2 <- 44 ;  sy2 <- 66
print("Posterior mean women without college degree", ( a+sy1 ) / ( b+n1 ) )
print("Posterior mode women without college degree", ( a+sy1 - 1)/(b+n1 ) )
print("Posterior 95% CI women without college degree", qgamma( c ( 0.025 , 0.975 ) , a+sy1 , b+n1 ) )
print("Posterior mean women with college degree", ( a+sy2 ) / ( b+n2 ) )
print("Posterior mode women with college degree", ( a+sy2 - 1)/(b+n2 ) )
print("Posterior 95% CI women with college degree", qgamma( c ( 0.025 , 0.975 ) , a+sy2 , b+n2 ) )
print("a": 3)
a=3
print("is":a)
print("is",a)
a <- 2 ; b <- 1
n1 <- 111 ; sy1 <- 217
n2 <- 44 ;  sy2 <- 66
print( paste0( "Posterior mean women without college degree", ( a+sy1 ) / ( b+n1 ) ))
print("Posterior mode women without college degree", ( a+sy1 - 1)/(b+n1 ) )
print("Posterior 95% CI women without college degree", qgamma( c ( 0.025 , 0.975 ) , a+sy1 , b+n1 ) )
print("Posterior mean women with college degree", ( a+sy2 ) / ( b+n2 ) )
print("Posterior mode women with college degree", ( a+sy2 - 1)/(b+n2 ) )
print("Posterior 95% CI women with college degree", qgamma( c ( 0.025 , 0.975 ) , a+sy2 , b+n2 ) )
a <- 2 ; b <- 1
n1 <- 111 ; sy1 <- 217
n2 <- 44 ;  sy2 <- 66
print( paste0( "Posterior mean women without college degree: ", ( a+sy1 ) / ( b+n1 ) ))
print( paste0( "Posterior mode women without college degree: ", ( a+sy1 - 1)/(b+n1 ) ))
print("Posterior 95% CI women without college degree", qgamma( c ( 0.025 , 0.975 ) , a+sy1 , b+n1 ) )
print( paste0( "Posterior mean women with college degree: ", ( a+sy2 ) / ( b+n2 ) ))
print(paste0( "Posterior mode women with college degree: ", ( a+sy2 - 1)/(b+n2 ) ))
print(paste0( "Posterior 95% CI women with college degree: ", qgamma( c ( 0.025 , 0.975 ) , a+sy2 , b+n2 ) ))
a <- 2 ; b <- 1
n1 <- 111 ; sy1 <- 217
n2 <- 44 ;  sy2 <- 66
print( paste0( "Posterior mean women without college degree: ", ( a+sy1 ) / ( b+n1 ) ))
print( paste0( "Posterior mode women without college degree: ", ( a+sy1 - 1)/(b+n1 ) ))
print( paste0(  "Posterior 95% CI women without college degree: ", qgamma( c ( 0.025 , 0.975 ) , a+sy1 , b+n1 ) ))
print( paste0( "Posterior mean women with college degree: ", ( a+sy2 ) / ( b+n2 ) ))
print(paste0( "Posterior mode women with college degree: ", ( a+sy2 - 1)/(b+n2 ) ))
print(paste0( "Posterior 95% CI women with college degree: ", qgamma( c ( 0.025 , 0.975 ) , a+sy2 , b+n2 ) ))
a <- 2 ; b <- 1
n1 <- 111 ; sy1 <- 217
n2 <- 44 ;  sy2 <- 66
print( paste0( "Posterior mean women without college degree: ", ( a+sy1 ) / ( b+n1 ) ))
print( paste0( "Posterior mode women without college degree: ", ( a+sy1 - 1)/(b+n1 ) ))
print( paste0(  "Posterior 95% CI women without college degree: ", qgamma( c ( 0.025 , 0.975 ) , a+sy1 , b+n1 ) ))
print( paste0( "Posterior mean women with college degree: ", ( a+sy2 ) / ( b+n2 ) ))
print(paste0( "Posterior mode women with college degree: ", ( a+sy2 - 1)/(b+n2 ) ))
print(paste0( "Posterior 95% CI women with college degree: ", qgamma( c ( 0.025 , 0.975 ) , a+sy2 , b+n2 ) ))
a <- 2 ; b <- 1
n1 <- 111 ; sy1 <- 217
n2 <- 44 ;  sy2 <- 66
print( paste0( "Posterior mean women without college degree: ", ( a+sy1 ) / ( b+n1 ) ))
print( paste0( "Posterior mode women without college degree: ", ( a+sy1 - 1)/(b+n1 ) ))
print( "Posterior 95% CI women without college degree: " )
print( qgamma( c ( 0.025 , 0.975 ) , a+sy1 , b+n1 ))
print( paste0( "Posterior mean women with college degree: ", ( a+sy2 ) / ( b+n2 ) ))
print(paste0( "Posterior mode women with college degree: ", ( a+sy2 - 1)/(b+n2 ) ))
print( "Posterior 95% CI women with college degree: " )
print( qgamma( c ( 0.025 , 0.975 ) , a+sy2 , b+n2 ))
par(mar=c(3,3,1,1),mgp=c(1.75,.75,0))
par(mfrow=c(1,2))
a<-2
b<-1
xtheta<-seq(0,5,length=1000)
plot(xtheta,dgamma(xtheta,a+s1,b+n1),type="l",col=gray(.5),xlab=expression(theta),
ylab=expression(paste(italic("p("),theta,"|",y[1],"...",y[n],")",sep="")))
lines(xtheta,dgamma(xtheta,a+s2,b+n2),col=gray(0),lwd=2)
lines(xtheta,dgamma(xtheta,a,b),type="l",lty=2,lwd=2)
abline(h=0,col="black")
y<-(0:12)
plot(y-.1, dnbinom(y, size=(a+s1), mu=(a+s1)/(b+n1)) , col=gray(.5) ,type="h",
ylab=expression(paste(italic("p("),y[n+1],"|",y[1],"...",y[n],")",sep="")),
xlab=expression(italic(y[n+1])),ylim=c(0,.35),lwd=3)
points(y+.1, dnbinom(y, size=(a+s2), mu=(a+s2)/(b+n2)) , col=gray(0) ,type="h",lwd=3)
legend(1,.375,legend=c("Less than bachelor's","Bachelor's or higher"),bty="n",
lwd=c(3,3),col=c(gray(.5),gray(0)))
rgamma
rgamma(100)
rgamma(100, a+sy1, b+n1)
theta_1_samples <- rgamma(1000, a+sy1, b+n1)
theta_2_samples <- rgamma(1000, a+sy2, b+n2)
mean(theta_1_samples > theta_2_samples)
theta_1_samples <- rgamma(1000, a+sy1, b+n1)
theta_2_samples <- rgamma(1000, a+sy2, b+n2)
mean(theta_1_samples > theta_2_samples)
theta_1_samples <- rgamma(1000, a+sy1, b+n1)
theta_2_samples <- rgamma(1000, a+sy2, b+n2)
mean(theta_1_samples > theta_2_samples)
theta_1_samples <- rgamma(10000, a+sy1, b+n1)
theta_2_samples <- rgamma(10000, a+sy2, b+n2)
mean(theta_1_samples > theta_2_samples)
theta_1_samples <- rgamma(10000, a+sy1, b+n1)
theta_2_samples <- rgamma(10000, a+sy2, b+n2)
mean(theta_1_samples > theta_2_samples)
theta_1_samples <- rgamma(10000, a+sy1, b+n1)
theta_2_samples <- rgamma(10000, a+sy2, b+n2)
mean(theta_1_samples > theta_2_samples)
theta_1_samples <- rgamma(10000, a+sy1, b+n1)
theta_2_samples <- rgamma(10000, a+sy2, b+n2)
mean(theta_1_samples > theta_2_samples)
theta_1_samples <- rgamma(10000, a+sy1, b+n1)
theta_2_samples <- rgamma(10000, a+sy2, b+n2)
mean(theta_1_samples > theta_2_samples)
theta_1_samples <- rgamma(10000, a+sy1, b+n1)
theta_2_samples <- rgamma(10000, a+sy2, b+n2)
mean(theta_1_samples > theta_2_samples)
theta_1_samples <- rgamma(10000, a+sy1, b+n1)
theta_2_samples <- rgamma(10000, a+sy2, b+n2)
mean(theta_1_samples > theta_2_samples)
theta_1_samples <- rgamma(100000, a+sy1, b+n1)
theta_2_samples <- rgamma(100000, a+sy2, b+n2)
mean(theta_1_samples > theta_2_samples)
theta_1_samples <- rgamma(100000, a+sy1, b+n1)
theta_2_samples <- rgamma(100000, a+sy2, b+n2)
mean(theta_1_samples > theta_2_samples)
theta_1_samples <- rgamma(100000, a+sy1, b+n1)
theta_2_samples <- rgamma(100000, a+sy2, b+n2)
mean(theta_1_samples > theta_2_samples)
theta_1_samples <- rgamma(100000, a+sy1, b+n1)
theta_2_samples <- rgamma(100000, a+sy2, b+n2)
mean(theta_1_samples > theta_2_samples)
theta_1_samples <- rgamma(100000, a+sy1, b+n1)
theta_2_samples <- rgamma(100000, a+sy2, b+n2)
mean(theta_1_samples > theta_2_samples)
theta_1_samples <- rgamma(100000, a+sy1, b+n1)
theta_2_samples <- rgamma(100000, a+sy2, b+n2)
mean(theta_1_samples > theta_2_samples)
par(mar=c(3,3,1,1),mgp=c(1.75,.75,0))
#par(mfrow=c(1,2))
a<-2
b<-1
xtheta<-seq(0,5,length=1000)
plot(xtheta,dgamma(xtheta,a+s1,b+n1),type="l",col=gray(.5),xlab=expression(theta),
ylab=expression(paste(italic("p("),theta,"|",y[1],"...",y[n],")",sep="")))
lines(xtheta,dgamma(xtheta,a+s2,b+n2),col=gray(0),lwd=2)
lines(xtheta,dgamma(xtheta,a,b),type="l",lty=2,lwd=2)
abline(h=0,col="black")
y<-(0:12)
plot(y-.1, dnbinom(y, size=(a+s1), mu=(a+s1)/(b+n1)) , col=gray(.5) ,type="h",
ylab=expression(paste(italic("p("),y[n+1],"|",y[1],"...",y[n],")",sep="")),
xlab=expression(italic(y[n+1])),ylim=c(0,.35),lwd=3)
points(y+.1, dnbinom(y, size=(a+s2), mu=(a+s2)/(b+n2)) , col=gray(0) ,type="h",lwd=3)
legend(1,.375,legend=c("Less than bachelor's","Bachelor's or higher"),bty="n",
lwd=c(3,3),col=c(gray(.5),gray(0)))
